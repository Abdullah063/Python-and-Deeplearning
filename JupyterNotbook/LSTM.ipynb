{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "46381bde-8ae7-4f5c-bc5b-c317289caa55",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "import torchvision\n",
    "import torchvision.transforms as transforms\n",
    "from torch.utils.data import DataLoader"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "a6218167-af79-46c9-9a06-877cb0b358f5",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "79318c30-4fbd-4866-9143-593db77c97a8",
   "metadata": {},
   "outputs": [],
   "source": [
    "input_dim = 28       # Giriş boyutu (her zaman adımı 28 piksel)\n",
    "hidden_dim = 128     # Gizli katman boyutu\n",
    "layer_dim = 2        # 2 katmanlı LSTM\n",
    "output_dim = 10      # 10 sınıf (0-9 rakamları)\n",
    "sequence_length = 28 # Zaman adımı (her resim 28x28 olduğundan)\n",
    "num_epochs = 5\n",
    "batch_size = 100\n",
    "learning_rate = 0.001  # Adam ile daha stabil öğrenme"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "5c8db5fe-57a3-4fcf-87e4-f6b6148b887b",
   "metadata": {},
   "outputs": [],
   "source": [
    "transform = transforms.Compose([\n",
    "    transforms.ToTensor(),\n",
    "    transforms.Normalize((0.5,), (0.5,))\n",
    "])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "266b450f-47cd-4024-9967-ea9096c5e0bf",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_dataset = torchvision.datasets.MNIST(root='./data', train=True, transform=transform, download=True)\n",
    "test_dataset = torchvision.datasets.MNIST(root='./data', train=False, transform=transform, download=True)\n",
    "\n",
    "train_loader = DataLoader(dataset=train_dataset, batch_size=batch_size, shuffle=True)\n",
    "test_loader = DataLoader(dataset=test_dataset, batch_size=batch_size, shuffle=False)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "b67e5c99-a4e3-41c8-a7c1-81748d0523b4",
   "metadata": {},
   "outputs": [],
   "source": [
    "class LSTMModel(nn.Module):\n",
    "    def __init__(self, input_dim, hidden_dim, layer_dim, output_dim):\n",
    "        super(LSTMModel, self).__init__()\n",
    "        \n",
    "        # Katman ve Boyutları\n",
    "        self.hidden_dim = hidden_dim\n",
    "        self.layer_dim = layer_dim\n",
    "        \n",
    "        # LSTM Katmanı\n",
    "        self.lstm = nn.LSTM(input_dim, hidden_dim, layer_dim, batch_first=True)\n",
    "        \n",
    "        # Tam Bağlantılı (Fully Connected) Çıkış Katmanı\n",
    "        self.fc = nn.Linear(hidden_dim, output_dim)\n",
    "\n",
    "    def forward(self, x):\n",
    "        # Başlangıç gizli durumu ve hücre durumu (GPU için .to(device) ekledik)\n",
    "        h0 = torch.zeros(self.layer_dim, x.size(0), self.hidden_dim).to(device)\n",
    "        c0 = torch.zeros(self.layer_dim, x.size(0), self.hidden_dim).to(device)\n",
    "\n",
    "        # LSTM ileri yayılım (forward propagation)\n",
    "        out, _ = self.lstm(x, (h0, c0))  # out: (batch_size, seq_length, hidden_dim)\n",
    "        \n",
    "        # Son zaman adımındaki çıktıyı al ve tam bağlı katmana ver\n",
    "        out = self.fc(out[:, -1, :])  # Son zaman adımındaki çıktı\n",
    "        return out"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "761041aa-81f4-4306-9cee-d9561c0b08f2",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Modeli Tanımlama\n",
    "model = LSTMModel(input_dim, hidden_dim, layer_dim, output_dim).to(device)\n",
    "criterion = nn.CrossEntropyLoss()\n",
    "optimizer = optim.Adam(model.parameters(), lr=learning_rate)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "114be0e5-015a-40de-bc9e-60a58128c78f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [1/5], Step [100/600], Loss: 0.7130\n",
      "Epoch [1/5], Step [200/600], Loss: 0.2594\n",
      "Epoch [1/5], Step [300/600], Loss: 0.2172\n",
      "Epoch [1/5], Step [400/600], Loss: 0.0794\n",
      "Epoch [1/5], Step [500/600], Loss: 0.2335\n",
      "Epoch [1/5], Step [600/600], Loss: 0.1992\n",
      "Epoch [2/5], Step [100/600], Loss: 0.0606\n",
      "Epoch [2/5], Step [200/600], Loss: 0.1028\n",
      "Epoch [2/5], Step [300/600], Loss: 0.0315\n",
      "Epoch [2/5], Step [400/600], Loss: 0.1238\n",
      "Epoch [2/5], Step [500/600], Loss: 0.1327\n",
      "Epoch [2/5], Step [600/600], Loss: 0.0662\n",
      "Epoch [3/5], Step [100/600], Loss: 0.0077\n",
      "Epoch [3/5], Step [200/600], Loss: 0.0635\n",
      "Epoch [3/5], Step [300/600], Loss: 0.0241\n",
      "Epoch [3/5], Step [400/600], Loss: 0.0795\n",
      "Epoch [3/5], Step [500/600], Loss: 0.0164\n",
      "Epoch [3/5], Step [600/600], Loss: 0.0293\n",
      "Epoch [4/5], Step [100/600], Loss: 0.0378\n",
      "Epoch [4/5], Step [200/600], Loss: 0.0921\n",
      "Epoch [4/5], Step [300/600], Loss: 0.0656\n",
      "Epoch [4/5], Step [400/600], Loss: 0.0656\n",
      "Epoch [4/5], Step [500/600], Loss: 0.0844\n",
      "Epoch [4/5], Step [600/600], Loss: 0.0161\n",
      "Epoch [5/5], Step [100/600], Loss: 0.0132\n",
      "Epoch [5/5], Step [200/600], Loss: 0.0563\n",
      "Epoch [5/5], Step [300/600], Loss: 0.0054\n",
      "Epoch [5/5], Step [400/600], Loss: 0.0249\n",
      "Epoch [5/5], Step [500/600], Loss: 0.1072\n",
      "Epoch [5/5], Step [600/600], Loss: 0.0196\n"
     ]
    }
   ],
   "source": [
    "for epoch in range(num_epochs):\n",
    "    for i, (images, labels) in enumerate(train_loader):\n",
    "        images = images.view(-1, sequence_length, input_dim).to(device)  # [batch_size, seq_length, input_size]\n",
    "        labels = labels.to(device)\n",
    "\n",
    "        # Gradients sıfırlama\n",
    "        optimizer.zero_grad()\n",
    "\n",
    "        # Forward propagation\n",
    "        outputs = model(images)\n",
    "\n",
    "        # Loss hesaplama\n",
    "        loss = criterion(outputs, labels)\n",
    "\n",
    "        # Backward propagation\n",
    "        loss.backward()\n",
    "\n",
    "        # Parametre güncelleme\n",
    "        optimizer.step()\n",
    "\n",
    "        if (i + 1) % 100 == 0:\n",
    "            print(f\"Epoch [{epoch+1}/{num_epochs}], Step [{i+1}/{len(train_loader)}], Loss: {loss.item():.4f}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "f1e63fb8-c6f9-4b7e-98e6-a4451411dbad",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Test Accuracy: 98.40%\n",
      "Eğitim Tamamlandı!\n"
     ]
    }
   ],
   "source": [
    "model.eval()\n",
    "with torch.no_grad():\n",
    "    correct = 0\n",
    "    total = 0\n",
    "    for images, labels in test_loader:\n",
    "        images = images.view(-1, sequence_length, input_dim).to(device)\n",
    "        labels = labels.to(device)\n",
    "        \n",
    "        outputs = model(images)\n",
    "        _, predicted = torch.max(outputs.data, 1)\n",
    "        \n",
    "        total += labels.size(0)\n",
    "        correct += (predicted == labels).sum().item()\n",
    "\n",
    "    print(f\"Test Accuracy: {100 * correct / total:.2f}%\")\n",
    "\n",
    "print(\"Eğitim Tamamlandı!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3c449b8b-4fa4-4860-84e9-80974846d424",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python (AllProject)",
   "language": "python",
   "name": "allproject"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
